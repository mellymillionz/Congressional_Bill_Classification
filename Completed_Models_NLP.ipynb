{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Testing** - Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from sodapy import Socrata\n",
    "import sqlalchemy as db\n",
    "\n",
    "import config_final as config\n",
    "from schema import DbSchema\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_db = DbSchema(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query all titles and Passing\n",
    "\n",
    "df = bills_db.query(\"\"\"\n",
    "    SELECT\n",
    "        cb.Title,\n",
    "        cb.PassH\n",
    "    FROM con_bills.current_bills as cb\n",
    "    JOIN con_bills.topics as tp\n",
    "    ON cb.BillID = tp.BillID\n",
    "    WHERE cb.Cong >=110\n",
    "    \"\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PassH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tokenizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "nlp.Defaults.stop_words |= {\"bill\",\"amend\", \"purpose\", \"united\", \"state\", \"states\", \"secretary\", \"act\", \"federal\", \"provide\"}\n",
    "\n",
    "replace_with_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "just_words = re.compile('[^a-zA-Z\\s]')\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "import string\n",
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    \n",
    "    \n",
    "    #lowercase everything\n",
    "    lower_text = text.lower()\n",
    "    \n",
    "    #remove punctuation\n",
    "#     no_pun_text = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #get rid of weird characters\n",
    "    text = replace_with_space.sub('',lower_text)\n",
    "    \n",
    "    #remove numbers\n",
    "    just_words_text = just_words.sub('', text)\n",
    "    \n",
    "    #add spacy tokenizer\n",
    "    mytokens = nlp(just_words_text, disable=['parser', 'ner'])\n",
    "#     print(mytokens)\n",
    "    \n",
    "    #for POS tagging\n",
    "#     mytokens = [word for word in mytokens if (word.pos_ == 'NOUN') or (word.pos_ == 'VERB') or (word.pos_ == 'ADJ') or (word.pos_ == 'ADV')]\n",
    "    \n",
    "    #lemmatize\n",
    "    mytokens = [word.lemma_.strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    #MAP SPECIFIC WORDS to others (veteran from veterans)\n",
    "\n",
    "    #add stopwords\n",
    "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "    \n",
    "    return mytokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Tokenizer to Title Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_title']=df['Title'].apply(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for_pickle = df.drop(columns='Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #is it vectorizer or transformed?\n",
    "# tk_titles = 'tokenized_titles.sav'\n",
    "# pickle.dump(for_pickle, open(tk_titles, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpickle Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "test_1 = pickle.load(open(tk_titles, 'rb'))\n",
    "test_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "\n",
    "Import packages:\n",
    "\n",
    "- CountVectorizer\n",
    "- TFIDF\n",
    "\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "\n",
    "**Remember to look at feature importances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction import FeatureHasher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test split** - Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = test_1['tokenized_title']\n",
    "y = test_1['PassH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test split** - Regular Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1 = df['Title']\n",
    "y1 = df['PassH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split!\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = .2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**\n",
    "\n",
    "- Precision means the percentage of your results which are relevant. \n",
    "- recall refers to the percentage of total relevant results correctly classified by your algorithm.\n",
    "\n",
    "Also make a precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(tokenizer = tokenizer, max_df = 0.90, max_features = 10000) # max_df=0.90, min_df=10\n",
    "# X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "# X_test_fit = vectorizer.transform(X_test)\n",
    "\n",
    "# print(len(vectorizer.get_feature_names()))\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer, max_df=0.90, max_features=1000)\n",
    "\n",
    "transformed = vectorizer.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorizer1 = CountVectorizer(tokenizer = tokenizer, max_df = 0.90, max_features = 10000) # max_df=0.90, min_df=10\n",
    "# trainsformed = vectorizer1.fit_transform(X_train, X_test)\n",
    "\n",
    "# print(len(vectorizer1.get_feature_names()))\n",
    "\n",
    "# vectorizer = CountVectorizer(tokenizer=tokenizer, max_df=0.5, max_features=None)\n",
    "\n",
    "# transformed = c_vectorizer.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #is it vectorizer or transformed?\n",
    "# filename_1 = 'finalized_countvectorizer_WORDSONLY_train.sav'\n",
    "# pickle.dump(X_train_transformed, open(filename_1, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #is it vectorizer or transformed?\n",
    "# filename_2 = 'finalized_countvectorizer_WORDSONLY_test.sav'\n",
    "# pickle.dump(X_test_fit, open(filename_2, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Pickle Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model from disk\n",
    "# X_train1 = pickle.load(open(filename_1, 'rb'))\n",
    "# X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model from disk\n",
    "# X_test1 = pickle.load(open(filename_2, 'rb'))\n",
    "\n",
    "# X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression CountVectorizer for Regular Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\"\"\"THIS IS ONLY FOR NON TOKENIZED TEXT\"\"\"\n",
    "\n",
    "lr_clf = Pipeline([('vect', CountVectorizer(tokenizer = dummy, preprocessor = dummy, max_df=0.5, max_features=1000)),\n",
    "               ('clf', LogisticRegression(class_weight='balanced', C=.8, random_state=2)),\n",
    "              ])\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "# lr_clf = LogisticRegression(class_weight='balanced', C=.8)\n",
    "\n",
    "lr_clf.fit(X_train1, y_train1)\n",
    "\n",
    "lr_y_pred = lr_clf.predict(X_test1)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test1, lr_y_pred))\n",
    "print(classification_report(y_test1, lr_y_pred))\n",
    "\n",
    "lr_confusion_matrix = confusion_matrix(y_test1, lr_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "lr_word_model = 'lr_model_NON_TOKENIZED_CV.sav'\n",
    "pickle.dump(lr_clf, open(lr_word_model, 'wb'))\n",
    " \n",
    "# some time later...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "Final_Model = pickle.load(open(lr_word_model, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = Final_Model.predict(['national, implement, forest, veteran'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression TFIDF for Regular Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Bring in OHE?\n",
    "\n",
    "lr_clf_tf = Pipeline([('vect', TfidfVectorizer(tokenizer=tokenizer, encoding='utf-8', smooth_idf = True)),\n",
    "               ('clf', LogisticRegression(class_weight='balanced', C=.8, random_state=2)),\n",
    "              ])\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "# lr_classifier = LogisticRegression()\n",
    "\n",
    "lr_clf_tf.fit(X_train1, y_train1)\n",
    "\n",
    "lr_y_pred_tf = lr_clf_tf.predict(X_test1)\n",
    "\n",
    "print(confusion_matrix(y_test1, lr_y_pred_tf))\n",
    "print(classification_report(y_test1, lr_y_pred_tf))\n",
    "\n",
    "lr_confusion_matrix = confusion_matrix(y_test, lr_y_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = lr_clf_tf.predict(['A bill to designate postal office post office for forests in a forest of national veterans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#shows percent data represented in each quadrant\n",
    "\n",
    "sns.heatmap(lr_confusion_matrix/np.sum(lr_confusion_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ROC/AUC Curve\n",
    "\n",
    "lr_dec = lr_clf.decision_function(X_test)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lr_roc_auc = roc_auc_score(y_test, lr_y_pred)\n",
    "lr_roc_auc\n",
    "\n",
    "#This is a decent ROC Score. Remember lays between .5 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, lr_dec)\n",
    "\n",
    "print('Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "\n",
    "plt.plot(test_fpr, test_tpr, color='darkorange',\n",
    "         lw=lw, label='Test ROC curve')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('(ROC) Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "lr_average_precision = average_precision_score(y_test, lr_dec)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      lr_average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(lr_clf, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(lr_average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Pre-Tokenized Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(x):\n",
    "    return x\n",
    "# Logistic Regression Classifier\n",
    "lr_classifier = LogisticRegression(tokenizer = dummy, preprocessor=dummy, class_weight = 'balanced', C=.8, random_state=2)\n",
    "\n",
    "lr_clf_pretok.fit(X_train, y_train)\n",
    "\n",
    "lr_y_pred_pt = lr_clf_pretok.predict(X_test1)\n",
    "\n",
    "print(confusion_matrix(y_test, lr_y_pred_pt))\n",
    "print(classification_report(y_test, lr_y_pred_pt))\n",
    "\n",
    "lr_confusion_matrix = confusion_matrix(y_test, lr_y_pred_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest** - Regular Text\n",
    "\n",
    "Class Imbalance: To overcome this issue, we used repeated random sub-sampling. Initially, we construct the testing data and the NoS training data sub-samples. For each disease, we train NoS classifiers and test all of them on the same data set. The final labels of the testing data are computed using a majority voting scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[('vectorizer', CountVectorizer(tokenizer = tokenizer)),\n",
    "                      ('classifier', RandomForestClassifier(max_depth=None, max_features='auto', n_estimators=100, class_weight=\"balanced\", max_df=0.90, max_features=1000))])\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train1, y_train1) \n",
    "\n",
    "rf_y_pred = rf_pipeline.predict(X_test1)\n",
    "\n",
    "print(confusion_matrix(y_test1, rf_y_pred))\n",
    "print(classification_report(y_test1, rf_y_pred))\n",
    "\n",
    "rf_confusion_matrix = confusion_matrix(y_test1, rf_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#shows percent data represented in each quadrant\n",
    "\n",
    "sns.heatmap(rf_confusion_matrix/np.sum(rf_confusion_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = rf_pipeline.named_steps['preprocessor'].transformers_[1][1]\\\n",
    "   .named_steps['onehot'].get_feature_names(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_pipeline.steps[1][1].feature_importances_\n",
    "len(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "top_k = 10\n",
    "new_indices = indices[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    \n",
    "#     n_features = importances.shape\n",
    "    \n",
    "    plt.figure(figsize=(15,200))\n",
    "    plt.barh(range(1044), importances, align='center') \n",
    "    \n",
    "    plt.yticks(np.arange(1044), feature_names) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(rf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing all Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
